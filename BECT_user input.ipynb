{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fa87b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/root/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [45:58<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [46:15<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [46:16<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [46:16<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [46:15<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [46:16<00:00,  7.20it/s]\n",
      "100%|██████████| 20000/20000 [11:40<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Token-level Accuracy: 0.9393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mt5-bangla-correction/tokenizer_config.json',\n",
       " 'mt5-bangla-correction/special_tokens_map.json',\n",
       " 'mt5-bangla-correction/spiece.model',\n",
       " 'mt5-bangla-correction/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"train.csv\").dropna()\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class BanglaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = \"translate Bangla to Bangla: \" + self.data.iloc[index][\"Input\"]\n",
    "        target = self.data.iloc[index][\"Target\"]\n",
    "\n",
    "        source_enc = self.tokenizer(source, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        target_enc = self.tokenizer(target, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": source_enc.attention_mask.squeeze(),\n",
    "            \"labels\": target_enc.input_ids.squeeze()\n",
    "        }\n",
    "\n",
    "train_data = BanglaDataset(df, tokenizer)\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()\n",
    "total_correct_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        mask = labels != tokenizer.pad_token_id\n",
    "        correct = (logits == labels) & mask\n",
    "\n",
    "        total_correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "final_accuracy = total_correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "print(f\"\\nFinal Token-level Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "model.save_pretrained(\"mt5-bangla-correction\")\n",
    "tokenizer.save_pretrained(\"mt5-bangla-correction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89f2409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:47<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0656\n",
      "Fine-tune Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:46<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0320\n",
      "Fine-tune Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:45<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0581\n",
      "Fine-tune Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:45<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:26<00:00, 29.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Token-level Accuracy after Fine-tuning: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mt5-bangla-finetuned/tokenizer_config.json',\n",
       " 'mt5-bangla-finetuned/special_tokens_map.json',\n",
       " 'mt5-bangla-finetuned/spiece.model',\n",
       " 'mt5-bangla-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"val.csv\").dropna()\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"mt5-bangla-correction\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"mt5-bangla-correction\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class BanglaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = \"translate Bangla to Bangla: \" + self.data.iloc[index][\"Input\"]\n",
    "        target = self.data.iloc[index][\"Target\"]\n",
    "\n",
    "        source_enc = self.tokenizer(source, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        target_enc = self.tokenizer(target, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": source_enc.attention_mask.squeeze(),\n",
    "            \"labels\": target_enc.input_ids.squeeze()\n",
    "        }\n",
    "\n",
    "val_data = BanglaDataset(df, tokenizer)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(4):\n",
    "    print(f\"Fine-tune Epoch {epoch+1}\")\n",
    "    for batch in tqdm(val_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "total_correct_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        mask = labels != tokenizer.pad_token_id\n",
    "        correct = (logits == labels) & mask\n",
    "\n",
    "        total_correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "accuracy = total_correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "print(f\"\\nFinal Token-level Accuracy after Fine-tuning: {accuracy:.4f}\")\n",
    "\n",
    "model.save_pretrained(\"mt5-bangla-finetuned\")\n",
    "tokenizer.save_pretrained(\"mt5-bangla-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756344c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence:\n",
      "এই সিদডান্ত বাসটবায়ন অগরদূট ছলেন সিটির ফভরণর পাইস পার্কার ।\n",
      "\n",
      "Corrected Sentence:\n",
      "এই সিদ্ধান্ত বাস্তবায়ন অগ্রদূট ছিলেন সিটির ফ্ল্যাটের পাইস পার্কার ।\n",
      "\n",
      "Difference:\n",
      "  এ\n",
      "  ই\n",
      "   \n",
      "  স\n",
      "  ি\n",
      "  দ\n",
      "- ড\n",
      "+ ্\n",
      "+ ধ\n",
      "  া\n",
      "  ন\n",
      "  ্\n",
      "  ত\n",
      "   \n",
      "  ব\n",
      "  া\n",
      "  স\n",
      "- ট\n",
      "+ ্\n",
      "+ ত\n",
      "  ব\n",
      "  া\n",
      "- য়\n",
      "+ য\n",
      "+ ়\n",
      "  ন\n",
      "   \n",
      "  অ\n",
      "  গ\n",
      "+ ্\n",
      "  র\n",
      "  দ\n",
      "  ূ\n",
      "  ট\n",
      "   \n",
      "  ছ\n",
      "+ ি\n",
      "  ল\n",
      "  ে\n",
      "  ন\n",
      "   \n",
      "  স\n",
      "  ি\n",
      "  ট\n",
      "  ি\n",
      "  র\n",
      "   \n",
      "  ফ\n",
      "- ভ\n",
      "- র\n",
      "- ণ\n",
      "+ ্\n",
      "+ ল\n",
      "+ ্\n",
      "+ য\n",
      "+ া\n",
      "+ ট\n",
      "+ ে\n",
      "  র\n",
      "   \n",
      "  প\n",
      "  া\n",
      "  ই\n",
      "  স\n",
      "   \n",
      "  প\n",
      "  া\n",
      "  র\n",
      "  ্\n",
      "  ক\n",
      "  া\n",
      "  র\n",
      "   \n",
      "  ।\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "import difflib\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"mt5-bangla-finetuned\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"mt5-bangla-finetuned\")\n",
    "model.eval()\n",
    "\n",
    "incorrect_sentence = \"এই সিদডান্ত বাসটবায়ন অগরদূট ছলেন সিটির ফভরণর পাইস পার্কার ।\"\n",
    "\n",
    "input_text = \"translate Bangla to Bangla: \" + incorrect_sentence\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, padding=True, truncation=True)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=128)\n",
    "corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Incorrect Sentence:\")\n",
    "print(incorrect_sentence)\n",
    "\n",
    "print(\"\\nCorrected Sentence:\")\n",
    "print(corrected_sentence)\n",
    "\n",
    "print(\"\\nDifference:\")\n",
    "diff = '\\n'.join(difflib.ndiff(incorrect_sentence, corrected_sentence))\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Bangla sentence (or 'exit'):  এতি ওবশা পুড়োনো অেভাস ।\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corrected Sentence:\n",
      "এটি অবশ্য পুরোনো অভ্যাস ।\n",
      "\n",
      "Difference:\n",
      "  এ\n",
      "- ত\n",
      "+ ট\n",
      "  ি\n",
      "   \n",
      "- ও\n",
      "+ অ\n",
      "  ব\n",
      "  শ\n",
      "- া\n",
      "+ ্\n",
      "+ য\n",
      "   \n",
      "  প\n",
      "  ু\n",
      "- ড়\n",
      "+ র\n",
      "  ো\n",
      "  ন\n",
      "  ো\n",
      "   \n",
      "  অ\n",
      "- ে\n",
      "  ভ\n",
      "+ ্\n",
      "+ য\n",
      "  া\n",
      "  স\n",
      "   \n",
      "  ।\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "import difflib\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"mt5-bangla-finetuned\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"mt5-bangla-finetuned\")\n",
    "model.eval()\n",
    "\n",
    "def correct_sentence(sentence):\n",
    "    input_text = \"translate Bangla to Bangla: \" + sentence\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def show_diff(orig, corrected):\n",
    "    return '\\n'.join(difflib.ndiff(orig, corrected))\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter Bangla sentence (or 'exit'): \")\n",
    "    if user_input.strip().lower() == 'exit':\n",
    "        break\n",
    "    corrected = correct_sentence(user_input)\n",
    "    print(f\"\\nCorrected Sentence:\\n{corrected}\")\n",
    "    print(f\"\\nDifference:\\n{show_diff(user_input, corrected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bd877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
